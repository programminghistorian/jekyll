---
title: Listen to your data: An Introduction to Sonfication with Python
authors:
- Shawn Graham
date: 2016-01-26
reviewers:
- n/a
layout: default
---

#_Poetry_

We try to see the past. There are any number of guides that will help you _visualize_ that which can't be seen. But perhaps we are too tied to our screens, too much invested in ‘seeing’. While there is a deep history and literature on archaeoacoustics and soundscapes that try to capture the sound of a place ([see for instance the Virtual St. Paul's](http://www.digitalstudies.org/ojs/index.php/digital_studies/article/view/251/310), I am interested in ’sonifying’ the data themselves. I want to figure out a grammar for representing data in sound that is appropriate for history. Drucker reminds us that ‘data’ are not really things given, but rather things captured, things transformed: that is to say, ‘capta’. In sonifying data, I literally perform the past in the present, and so the assumptions, the transformations, I make are foregrounded. The resulting aural experience is a literal ‘deformance’ (portmanteua of ‘deform’ and ‘perform’) that makes us consider the past in a new way.

I want to hear the meaning of the past. But I know that I can’t; nevertheless, when I hear an instrument, I can imagine the physicality of the player playing it; in its echoes and resonances I can discern the physical space. I can feel the bass; I can move to the rhythm. The music engages my whole body, my whole imagination. Its associations with sounds, music, and tones I’ve heard before create a deep temporal experience, a system of embodied relationships. Visual? We have had visual representations of the past for so long, we have almost forgotten the artistic and performative aspect of those grammars of expression.

In this tutorial, you will learn to make some noise from your data about the past. The _meaning_ of that noise, well... that's up to you.

# Objectives

In this tutorial, I will introduce you to three different ways of generating sound or music from your data. In the first, we will use a freely available and free-to-use system developed by XXXXXX called 'Musicalgorithms', to introduce some of the issues and key terms involved. In the second, we will use a small python library to 'parameter map' our data against the 88 key keyboard, and introduce some artistry into our work. Finally, we will learn how to load our data into the open source live-coding environment for sound and music, Sonic Pi, at which time I will leave you to explore that project's tutorials and resources. 

You will see that 'sonification' moves us along the spectrum from mere 'visualization/auralization' to actual performance.

## Terms

+ MIDI = musical instrument digital interface

